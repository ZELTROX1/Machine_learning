{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install treenode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYU0u6lhcPR5",
        "outputId": "db054c06-216b-458a-8f61-5697d39969ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting treenode\n",
            "  Downloading treenode-0.1.5.zip (6.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: treenode\n",
            "  Building wheel for treenode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for treenode: filename=treenode-0.1.5-py3-none-any.whl size=3865 sha256=82ece1b965e7094969e2a7b21ce0019ada26030e0a3e22cee44caeabf9472c8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/80/9d/5f041fe62b5136b1109f25f95e3e80dc1b3e6cba867930b576\n",
            "Successfully built treenode\n",
            "Installing collected packages: treenode\n",
            "Successfully installed treenode-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CBhsAGvpUgQm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from treenode import TreeNode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TreeNode:\n",
        "    def __init__(self,feature_idx, feature_val, prediction_probs, information_gain) -> None:\n",
        "        self.feature_idx = feature_idx\n",
        "        self.feature_val = feature_val\n",
        "        self.prediction_probs = prediction_probs\n",
        "        self.information_gain = information_gain\n",
        "        self.left = None\n",
        "        self.right = None"
      ],
      "metadata": {
        "id": "8vXMecA1nRHL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decision_tree:\n",
        "  def __init__(self,max_depth=4,min_samples_leaf=1,min_information_gain=0.0):\n",
        "    self.max_depth = max_depth\n",
        "    self.min_samples_leaf = min_samples_leaf\n",
        "    self.min_information_gain = min_information_gain\n",
        "    self.tree = None\n",
        "  def entropy(self,class_probablities:list)->float:\n",
        "    return -sum([p*np.log2(p) for p in class_probablities if p>0])\n",
        "\n",
        "  def class_probablities(self,y:list)->list:\n",
        "    total_count = len(y)\n",
        "    # y ⁵means labels\n",
        "    return [count/total_count for count in Counter(y).values()]\n",
        "\n",
        "  def data_entropy(self,y:list)->float:\n",
        "    return self.entropy(self.class_probablities(y))\n",
        "\n",
        "  def _partition_entropy(self, subsets: list) -> float:\n",
        "        \"\"\"Returns the entropy from this partition of data into subsets\"\"\"\n",
        "        total_count = sum([len(subset) for subset in subsets])\n",
        "        return sum([self.data_entropy(subset) * (len(subset) / total_count) for subset in subsets])\n",
        "  def _split_data(self,data:np.ndarray,split_column:int,split_value:float)->tuple:\n",
        "    a = data[:,split_column]\n",
        "    left_data = data[a<=split_value]\n",
        "    right_data = data[a>split_value]\n",
        "    return left_data,right_data\n",
        "\n",
        "  def select_feature(self, data: np.ndarray) -> int:\n",
        "    feature_idx = list(range(data.shape[1] - 1))\n",
        "    if self.numb_of_features_splitting == \"sqrt\":\n",
        "        feature_idx_to_use = np.random.choice(feature_idx, size=int(np.sqrt(len(feature_idx))))\n",
        "    elif self.numb_of_features_splitting == \"log2\":\n",
        "        feature_idx_to_use = np.random.choice(feature_idx, size=int(np.log2(len(feature_idx))))\n",
        "    else:\n",
        "        feature_idx_to_use = feature_idx\n",
        "    return np.random.choice(feature_idx_to_use)\n",
        "\n",
        "  def best_split(self,data:np.ndarray)->dict:\n",
        "    min_part_entropy = 1e6\n",
        "    min_entropy_freature_idx = None\n",
        "    min_entropy_freature_value = None\n",
        "    for freature_idx in range(data.shape[1]-1):\n",
        "      feature_values = np.median(data[:,freature_idx])\n",
        "      left,right= self._split_data(data,freature_idx,feature_values)\n",
        "      part_entropy=self._partition_entropy([left[:,-1],right[:,-1]])\n",
        "      if  part_entropy < min_part_entropy:\n",
        "        min_part_entropy = part_entropy\n",
        "        min_entropy_freature_idx = freature_idx\n",
        "        min_entropy_freature_value = feature_values\n",
        "        left_min,right_min = left,right\n",
        "    return left_min,right_min,min_entropy_freature_idx,min_entropy_freature_value,min_part_entropy\n",
        "\n",
        "  def label_probabilities(self, data: np.array) -> np.array:\n",
        "    labels_as_int = data[:, -1].astype(int)\n",
        "    total_labels = len(labels_as_int)\n",
        "    label_probabilities = np.zeros(len(self.labels_in_train), dtype=float)\n",
        "    for i, label in enumerate(self.labels_in_train):\n",
        "        label_indices = np.where(labels_as_int == label)[0]\n",
        "        if len(label_indices) > 0:\n",
        "            label_probabilities[i] = len(label_indices) / total_labels\n",
        "    return label_probabilities\n",
        "\n",
        "  def create_tree(self, data: np.ndarray, current_depth=0) -> TreeNode:\n",
        "    if current_depth > self.max_depth:\n",
        "        return None\n",
        "\n",
        "    split_1_data, split_2_data, feature_idx, feature_value, split_entropy = self.best_split(data)\n",
        "\n",
        "    _labels_probabilities = self.label_probabilities(data)\n",
        "    node_entropy = self.data_entropy(data[:, -1])\n",
        "    information_gain = node_entropy - split_entropy\n",
        "\n",
        "    node = TreeNode(feature_idx, feature_value, _labels_probabilities, information_gain)\n",
        "\n",
        "    if split_1_data.shape[0] < self.min_samples_leaf or split_2_data.shape[0] < self.min_samples_leaf:\n",
        "        return node\n",
        "    if information_gain < self.min_information_gain:\n",
        "        return node\n",
        "\n",
        "    current_depth += 1\n",
        "    node.left = self.create_tree(split_1_data, current_depth)\n",
        "    node.right = self.create_tree(split_2_data, current_depth)\n",
        "    return node\n",
        "\n",
        "  def fit(self, X_train: np.array, y_train: np.array) -> None:\n",
        "    self.labels_in_train = np.unique(y_train)\n",
        "    train_data = np.concatenate((X_train, y_train.reshape(-1, 1)), axis=1)\n",
        "    self.tree = self.create_tree(train_data, current_depth=0)\n",
        "\n",
        "\n",
        "  def predict_one_sample(self, X: np.array) -> np.array:\n",
        "    node = self.tree\n",
        "    while node.left or node.right:\n",
        "        if X[node.feature_idx] <= node.feature_val:\n",
        "            node = node.left\n",
        "        else:\n",
        "            node = node.right\n",
        "    return node.prediction_probs\n",
        "\n",
        "  def predict(self, X: np.array) -> np.array:\n",
        "    pred_probs = np.array([self.predict_one_sample(x) for x in X])\n",
        "    return np.argmax(pred_probs, axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "SZoOUTudUxuA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "SaXiSOK6yMJy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "FcBUrgyGyQ2o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Decision_tree(max_depth=5)\n",
        "model.fit(x_train,y_train)\n",
        "y_pred=model.predict(x_test)\n",
        "accuracy_score(y_test,y_pred)\n",
        "# print_tree(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOoOUVsvx2dR",
        "outputId": "aabb909e-0faa-433b-dadd-d9253624d146"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9444444444444444"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}